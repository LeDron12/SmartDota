{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание обучения\n",
    "\n",
    "### Данные\n",
    "\n",
    "1. Берутся только id героев как фичи\n",
    "\n",
    "### Обучение\n",
    "\n",
    "1. Для **ML подходов:** \\\n",
    "Конкатинируются в вектор длины $ 2 * len(unique(id)) $ \\\n",
    "Для Radiant - $ len(unique(id)) $ \\\n",
    "Для Dire - $ len(unique(id)) $\n",
    "\n",
    "2. Для **DL подходов:** ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/ankamenskiy/SmartDota/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dataclasses\n",
    "import json\n",
    "\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.data.api.OpenDota.pro_matches_dataloader import ProMatchesDataloader\n",
    "from src.data.api.OpenDota.public_matches_dataloader import PublicMatchesDataloader\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Читаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Key\n",
      "Last match index: 7504377105\n",
      "Last match index: 7502042193\n",
      "108937 4129\n"
     ]
    }
   ],
   "source": [
    "train_path = '/Users/ankamenskiy/SmartDota/cache/public_110000_7-34b-ALL'\n",
    "test_path = '/Users/ankamenskiy/SmartDota/cache/pro_10000_dups_light'\n",
    "\n",
    "train_dataloader = PublicMatchesDataloader(0, 0)\n",
    "train_dataloader.load(path=train_path)\n",
    "test_dataloader = ProMatchesDataloader()\n",
    "test_dataloader.load(path=test_path)\n",
    "\n",
    "train_data = train_dataloader.data\n",
    "test_data = test_dataloader.data\n",
    "\n",
    "# ya nasral kogda kachal danniye\n",
    "unique = {}\n",
    "[unique.setdefault(elem.match_id, elem) for elem in test_data]\n",
    "test_data = [v for k, v in unique.items()]\n",
    "unique = {}\n",
    "[unique.setdefault(elem.match_id, elem) for elem in train_data]\n",
    "train_data = [v for k, v in unique.items()]\n",
    "\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Готовим датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For PUBLIC matches [TRAIN data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>radiant_hero_ids</th>\n",
       "      <th>dire_hero_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23078</th>\n",
       "      <td>1</td>\n",
       "      <td>[45, 76, 98, 63, 8]</td>\n",
       "      <td>[29, 72, 35, 14, 71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61909</th>\n",
       "      <td>0</td>\n",
       "      <td>[136, 10, 47, 7, 19]</td>\n",
       "      <td>[43, 90, 84, 129, 42]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77455</th>\n",
       "      <td>0</td>\n",
       "      <td>[15, 41, 98, 68, 105]</td>\n",
       "      <td>[1, 120, 114, 72, 26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37329</th>\n",
       "      <td>1</td>\n",
       "      <td>[13, 60, 128, 93, 53]</td>\n",
       "      <td>[41, 40, 20, 71, 56]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2368</th>\n",
       "      <td>1</td>\n",
       "      <td>[13, 20, 137, 105, 48]</td>\n",
       "      <td>[63, 43, 33, 86, 62]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target        radiant_hero_ids          dire_hero_ids\n",
       "23078       1     [45, 76, 98, 63, 8]   [29, 72, 35, 14, 71]\n",
       "61909       0    [136, 10, 47, 7, 19]  [43, 90, 84, 129, 42]\n",
       "77455       0   [15, 41, 98, 68, 105]  [1, 120, 114, 72, 26]\n",
       "37329       1   [13, 60, 128, 93, 53]   [41, 40, 20, 71, 56]\n",
       "2368        1  [13, 20, 137, 105, 48]   [63, 43, 33, 86, 62]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    44783\n",
       "1    50022\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_val = pd.DataFrame()\n",
    "\n",
    "df_train_val['target'] = [1 if match.radiant_win else 0 for match in train_data]\n",
    "df_train_val['radiant_hero_ids'] = [match.radiant_team for match in train_data]\n",
    "df_train_val['dire_hero_ids'] = [match.dire_team for match in train_data]\n",
    "\n",
    "df_train_val.dropna(inplace=True)\n",
    "display(df_train_val.sample(5))\n",
    "df_train_val.groupby('target')['target'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For PRO matches [TEST data]\n",
    "\n",
    "0 - Radiant \\\n",
    "1 - Dire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>radiant_hero_ids</th>\n",
       "      <th>dire_hero_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>0</td>\n",
       "      <td>[53, 85, 69, 106, 6]</td>\n",
       "      <td>[121, 14, 49, 76, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>0</td>\n",
       "      <td>[84, 86, 60, 46, 100]</td>\n",
       "      <td>[85, 43, 79, 35, 48]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3945</th>\n",
       "      <td>0</td>\n",
       "      <td>[71, 26, 20, 76, 48]</td>\n",
       "      <td>[102, 64, 96, 63, 106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1</td>\n",
       "      <td>[86, 64, 14, 10, 22]</td>\n",
       "      <td>[71, 72, 11, 28, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3231</th>\n",
       "      <td>0</td>\n",
       "      <td>[3, 20, 70, 96, 25]</td>\n",
       "      <td>[123, 29, 121, 73, 76]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      target       radiant_hero_ids           dire_hero_ids\n",
       "3733       0   [53, 85, 69, 106, 6]   [121, 14, 49, 76, 10]\n",
       "1466       0  [84, 86, 60, 46, 100]    [85, 43, 79, 35, 48]\n",
       "3945       0   [71, 26, 20, 76, 48]  [102, 64, 96, 63, 106]\n",
       "284        1   [86, 64, 14, 10, 22]    [71, 72, 11, 28, 23]\n",
       "3231       0    [3, 20, 70, 96, 25]  [123, 29, 121, 73, 76]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "target              0\n",
       "radiant_hero_ids    0\n",
       "dire_hero_ids       0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    1990\n",
       "1    2105\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb = [elem.picks_bans for elem in test_data]\n",
    "\n",
    "radiants, dires, targets = [], [], []\n",
    "\n",
    "for i, elem in enumerate(test_data):\n",
    "    pb = elem.picks_bans\n",
    "    target = elem.pro_match_data.radiant_win # 1 if RADIANT win | 0 if DIRE win\n",
    "\n",
    "    if pb is None or target is None:\n",
    "        continue\n",
    "    \n",
    "    pb = sorted(filter(lambda x: x.is_pick, pb), key=lambda x: x.team)\n",
    "    assert len(pb) == 10\n",
    "\n",
    "    radiants.append([e.hero_id for e in pb[:5]])\n",
    "    dires.append([e.hero_id for e in pb[5:]])\n",
    "    targets.append(int(target))\n",
    "\n",
    "df_test = pd.DataFrame()\n",
    "\n",
    "df_test['target'] = targets\n",
    "df_test['radiant_hero_ids'] = radiants\n",
    "df_test['dire_hero_ids'] = dires\n",
    "\n",
    "display(df_test.sample(5))\n",
    "display(df_test.isna().sum())\n",
    "df_test.groupby('target')['target'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If 248-dim OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94805, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>1_radiant</th>\n",
       "      <th>2_radiant</th>\n",
       "      <th>3_radiant</th>\n",
       "      <th>4_radiant</th>\n",
       "      <th>5_radiant</th>\n",
       "      <th>6_radiant</th>\n",
       "      <th>7_radiant</th>\n",
       "      <th>8_radiant</th>\n",
       "      <th>9_radiant</th>\n",
       "      <th>...</th>\n",
       "      <th>120_dire</th>\n",
       "      <th>121_dire</th>\n",
       "      <th>123_dire</th>\n",
       "      <th>126_dire</th>\n",
       "      <th>128_dire</th>\n",
       "      <th>129_dire</th>\n",
       "      <th>135_dire</th>\n",
       "      <th>136_dire</th>\n",
       "      <th>137_dire</th>\n",
       "      <th>138_dire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94806</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94807</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94814</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94815</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94819</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94805 rows × 249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  1_radiant  2_radiant  3_radiant  4_radiant  5_radiant  \\\n",
       "0           1          0          0          0          0          0   \n",
       "1           1          0          0          0          0          0   \n",
       "2           0          0          0          0          0          0   \n",
       "3           0          0          0          0          0          0   \n",
       "4           1          0          0          0          0          0   \n",
       "...       ...        ...        ...        ...        ...        ...   \n",
       "94806       1          0          0          0          0          0   \n",
       "94807       0          0          0          0          0          0   \n",
       "94814       1          0          0          0          0          0   \n",
       "94815       1          0          0          0          0          0   \n",
       "94819       0          0          0          0          0          0   \n",
       "\n",
       "       6_radiant  7_radiant  8_radiant  9_radiant  ...  120_dire  121_dire  \\\n",
       "0              0          0          0          0  ...         0         0   \n",
       "1              0          0          0          1  ...         0         0   \n",
       "2              0          0          0          0  ...         0         0   \n",
       "3              0          0          0          0  ...         0         0   \n",
       "4              0          0          1          0  ...         0         0   \n",
       "...          ...        ...        ...        ...  ...       ...       ...   \n",
       "94806          0          0          0          0  ...         0         0   \n",
       "94807          0          0          0          0  ...         0         0   \n",
       "94814          0          0          0          0  ...         0         0   \n",
       "94815          0          0          0          0  ...         0         0   \n",
       "94819          0          0          0          0  ...         0         0   \n",
       "\n",
       "       123_dire  126_dire  128_dire  129_dire  135_dire  136_dire  137_dire  \\\n",
       "0             0         0         0         0         0         0         0   \n",
       "1             1         0         0         0         1         0         0   \n",
       "2             0         0         0         0         0         0         0   \n",
       "3             0         0         0         0         0         0         1   \n",
       "4             0         0         0         0         0         0         1   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "94806         0         0         0         0         0         0         0   \n",
       "94807         0         0         0         0         0         0         0   \n",
       "94814         0         0         0         0         0         0         0   \n",
       "94815         0         0         0         0         0         0         0   \n",
       "94819         0         0         0         0         0         0         0   \n",
       "\n",
       "       138_dire  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "94806         0  \n",
       "94807         0  \n",
       "94814         0  \n",
       "94815         0  \n",
       "94819         0  \n",
       "\n",
       "[94805 rows x 249 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>1_radiant</th>\n",
       "      <th>2_radiant</th>\n",
       "      <th>3_radiant</th>\n",
       "      <th>4_radiant</th>\n",
       "      <th>5_radiant</th>\n",
       "      <th>6_radiant</th>\n",
       "      <th>7_radiant</th>\n",
       "      <th>8_radiant</th>\n",
       "      <th>9_radiant</th>\n",
       "      <th>...</th>\n",
       "      <th>120_dire</th>\n",
       "      <th>121_dire</th>\n",
       "      <th>123_dire</th>\n",
       "      <th>126_dire</th>\n",
       "      <th>128_dire</th>\n",
       "      <th>129_dire</th>\n",
       "      <th>135_dire</th>\n",
       "      <th>136_dire</th>\n",
       "      <th>137_dire</th>\n",
       "      <th>138_dire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4094 rows × 249 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  1_radiant  2_radiant  3_radiant  4_radiant  5_radiant  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          1          0          0          0          0          0   \n",
       "2          1          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          1   \n",
       "...      ...        ...        ...        ...        ...        ...   \n",
       "4090       1          0          0          0          0          0   \n",
       "4091       1          0          0          0          0          0   \n",
       "4092       0          0          0          0          0          0   \n",
       "4093       1          0          0          0          0          0   \n",
       "4094       0          0          0          0          0          0   \n",
       "\n",
       "      6_radiant  7_radiant  8_radiant  9_radiant  ...  120_dire  121_dire  \\\n",
       "0             0          0          0          0  ...         0         1   \n",
       "1             0          0          0          0  ...         0         0   \n",
       "2             0          0          0          0  ...         0         0   \n",
       "3             0          0          0          0  ...         0         0   \n",
       "4             0          0          0          0  ...         0         1   \n",
       "...         ...        ...        ...        ...  ...       ...       ...   \n",
       "4090          0          0          0          0  ...         0         0   \n",
       "4091          0          0          0          0  ...         0         0   \n",
       "4092          0          0          0          0  ...         0         0   \n",
       "4093          0          0          0          0  ...         0         0   \n",
       "4094          0          0          0          0  ...         0         0   \n",
       "\n",
       "      123_dire  126_dire  128_dire  129_dire  135_dire  136_dire  137_dire  \\\n",
       "0            0         1         0         0         1         0         0   \n",
       "1            0         0         1         1         0         0         0   \n",
       "2            0         0         0         0         0         0         0   \n",
       "3            1         0         0         0         0         0         0   \n",
       "4            0         0         0         0         0         0         0   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4090         0         0         0         0         0         0         0   \n",
       "4091         0         0         0         0         0         0         0   \n",
       "4092         0         0         0         0         0         0         0   \n",
       "4093         0         0         0         0         0         0         0   \n",
       "4094         0         0         0         0         0         0         0   \n",
       "\n",
       "      138_dire  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "4090         0  \n",
       "4091         0  \n",
       "4092         0  \n",
       "4093         0  \n",
       "4094         0  \n",
       "\n",
       "[4094 rows x 249 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "def make_hero_onehot(df):\n",
    "    df1 = df.join(\n",
    "            pd.DataFrame.sparse.from_spmatrix(\n",
    "                mlb.fit_transform(df.pop('radiant_hero_ids')),\n",
    "                index=df.index,\n",
    "                columns=mlb.classes_\n",
    "            )\n",
    "        )\n",
    "    # display(df1)\n",
    "\n",
    "    df2 = df1.join(\n",
    "            pd.DataFrame.sparse.from_spmatrix(\n",
    "                mlb.fit_transform(df1.pop('dire_hero_ids')),\n",
    "                index=df1.index,\n",
    "                columns=mlb.classes_\n",
    "            ),\n",
    "            lsuffix='_radiant', \n",
    "            rsuffix='_dire'\n",
    "        )\n",
    "    # display(df2)\n",
    "    \n",
    "    return df2\n",
    "\n",
    "print(df_train_val.shape)\n",
    "df_train_val = make_hero_onehot(df_train_val).drop_duplicates()\n",
    "df_test = make_hero_onehot(df_test).drop_duplicates()\n",
    "\n",
    "display(df_train_val)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If 124-dim OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19, 20: 20, 21: 21, 22: 22, 23: 23, 25: 24, 26: 25, 27: 26, 28: 27, 29: 28, 30: 29, 31: 30, 32: 31, 33: 32, 34: 33, 35: 34, 36: 35, 37: 36, 38: 37, 39: 38, 40: 39, 41: 40, 42: 41, 43: 42, 44: 43, 45: 44, 46: 45, 47: 46, 48: 47, 49: 48, 50: 49, 51: 50, 52: 51, 53: 52, 54: 53, 55: 54, 56: 55, 57: 56, 58: 57, 59: 58, 60: 59, 61: 60, 62: 61, 63: 62, 64: 63, 65: 64, 66: 65, 67: 66, 68: 67, 69: 68, 70: 69, 71: 70, 72: 71, 73: 72, 74: 73, 75: 74, 76: 75, 77: 76, 78: 77, 79: 78, 80: 79, 81: 80, 82: 81, 83: 82, 84: 83, 85: 84, 86: 85, 87: 86, 88: 87, 89: 88, 90: 89, 91: 90, 92: 91, 93: 92, 94: 93, 95: 94, 96: 95, 97: 96, 98: 97, 99: 98, 100: 99, 101: 100, 102: 101, 103: 102, 104: 103, 105: 104, 106: 105, 107: 106, 108: 107, 109: 108, 110: 109, 111: 110, 112: 111, 113: 112, 114: 113, 119: 114, 120: 115, 121: 116, 123: 117, 126: 118, 128: 119, 129: 120, 135: 121, 136: 122, 137: 123, 138: 124}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d759ea79b4347f7a702cff0aed9a753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5f47beba194a61a7b74d2fb05d191e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4091</th>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4092</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4093</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4094</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4095 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target  1  2  3  4  5  6  7  8  9  ...  115  116  117  118  119  120  \\\n",
       "0          0  0  0  0  0  0  0  0  0  0  ...    1   -1    0   -1    0    0   \n",
       "1          1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0   -1   -1   \n",
       "2          1  0  0  0  0  0  0  0  0 -1  ...    0    0    0    0    0    0   \n",
       "3          0  0  0  0  0  0  0  0  0  0  ...    0    0   -1    1    0    0   \n",
       "4          0  0  0  0  0  1  0  0  0  0  ...    0   -1    0    0    0    0   \n",
       "...      ... .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...   \n",
       "4090       1 -1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "4091       1 -1  0  0  0  0  0  0  0  0  ...    1    0    0    0    0    0   \n",
       "4092       0  0  0  0  0  0  0  0  0  0  ...    1    0    0    0    0    0   \n",
       "4093       1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    1   \n",
       "4094       0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "\n",
       "      121  122  123  124  \n",
       "0      -1    0    0    0  \n",
       "1       0    0    0    0  \n",
       "2       0    0    0    0  \n",
       "3       0    0    0    0  \n",
       "4       0    0    0    0  \n",
       "...   ...  ...  ...  ...  \n",
       "4090    0    0    0    0  \n",
       "4091    0    0    0    0  \n",
       "4092    0    0    0    0  \n",
       "4093    0    0    0    0  \n",
       "4094    0    0    0    0  \n",
       "\n",
       "[4095 rows x 125 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94800</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94801</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94802</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94803</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94804</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94805 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  1  2  3  4  5  6  7  8  9  ...  115  116  117  118  119  120  \\\n",
       "0           1  0  0  0  0  0  0 -1  0  0  ...    0    0    0    0    0    0   \n",
       "1           1  0  0  0  0  0  0  0  0  1  ...    0    0   -1    0    0    0   \n",
       "2           0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    1   \n",
       "3           0  0  0  0  0  0  0  0  0  0  ...    0    0    1    0    0    0   \n",
       "4           1  0  0  0  0  0 -1  0  1  0  ...    0    0    0    0    0    1   \n",
       "...       ... .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...   \n",
       "94800       1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "94801       0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "94802       1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    1    0   \n",
       "94803       1  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "94804       0 -1  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "\n",
       "       121  122  123  124  \n",
       "0        0    1    0    0  \n",
       "1       -1    0    0    0  \n",
       "2        0    0    0    0  \n",
       "3        0    0   -1    0  \n",
       "4        0    1   -1    0  \n",
       "...    ...  ...  ...  ...  \n",
       "94800    0    0    0    0  \n",
       "94801    0    0    0    1  \n",
       "94802    0    0    0    0  \n",
       "94803    0    0    0    0  \n",
       "94804    0    0    0    1  \n",
       "\n",
       "[94805 rows x 125 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_HEROES = 124\n",
    "\n",
    "with open('/Users/ankamenskiy/SmartDota/data/heroes.json', 'r') as f:\n",
    "    heroes = json.loads(f.read())\n",
    "    hero2pos = {hero['id']: i + 1 for i, hero in enumerate(heroes)}\n",
    "    print(hero2pos)\n",
    "\n",
    "def make_small_ohe(df):\n",
    "    columns = ['target'] + [str(i + 1) for i in range(NUM_HEROES)]\n",
    "    ret = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        r = [row['target']] + [0]*NUM_HEROES\n",
    "\n",
    "        for id in row['radiant_hero_ids']:\n",
    "            r[hero2pos[id]] = 1\n",
    "        for id in row['dire_hero_ids']:\n",
    "            r[hero2pos[id]] = -1\n",
    "        \n",
    "        ret.append({col: val for col, val in zip(columns, r)})\n",
    "\n",
    "    return pd.DataFrame(ret)\n",
    "\n",
    "df_test_extended = make_small_ohe(df_test)\n",
    "df_train_val_extended = make_small_ohe(df_train_val)\n",
    "\n",
    "display(df_test_extended)\n",
    "display(df_train_val_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "X = df_train_val_extended.drop(['target'], axis=1).to_numpy()\n",
    "y = df_train_val_extended['target'].to_numpy()\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=RANDOM_STATE)\n",
    "\n",
    "X_test = df_test_extended.drop(['target'], axis=1).to_numpy()\n",
    "y_test = df_test_extended['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prob_hist(probas):\n",
    "    dire_win, radiant_win = probas[:, 0], probas[:, 1]\n",
    "    \n",
    "    plt.hist(dire_win, color='r', alpha = 0.3, bins=40)\n",
    "    plt.hist(radiant_win, color='g', alpha = 0.6, bins=40)\n",
    "\n",
    "def compare_results(y_pred, y_true, X_val):\n",
    "    df = pd.DataFrame()\n",
    "    df['y_pred'] = y_pred\n",
    "    df['y_true'] = y_true\n",
    "    display(df)\n",
    "\n",
    "def make_prediction(model, heroes):\n",
    "    df = pd.DataFrame()\n",
    "    df['radiant_hero_ids'] = heroes['radiant']\n",
    "    df['dire_hero_ids'] = heroes['dire']\n",
    "\n",
    "    df = make_hero_onehot(df)\n",
    "    X_pred = df.to_numpy()\n",
    "\n",
    "    probas = model.predict_proba(X_pred)\n",
    "    print(probas)\n",
    "    probas = np.exp(probas) / np.sum(np.exp(probas)) # softmax\n",
    "\n",
    "    return {\n",
    "        'dire': probas[0],\n",
    "        'radiant': probas[1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LogReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разобраться с валидацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression(C=0.3, class_weight='balanced', max_iter=200, penalty=None, solver='sag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.60631e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.39194e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.08434e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.93256e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.65144e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.13736e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.63085e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.84062e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.90773e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.52826e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.83578e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.94656e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=9.87446e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.94467e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.71268e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.73394e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.6408e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.65905e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.41368e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.55846e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.39194e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.60631e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.08434e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.93256e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.65144e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.13736e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.63085e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.84062e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.52826e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.90773e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.83578e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.94656e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=9.87446e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.94467e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.71268e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.73394e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.65905e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.6408e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.55846e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.41368e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.60631e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.08434e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.39194e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.93256e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.65144e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.63085e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.13736e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.84062e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.90773e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.52826e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=5.83578e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=4.94656e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=9.87446e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=3.94467e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=2.71268e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.73394e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.65905e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.6408e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=1.41368e-17): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_glm/_newton_solver.py:498: LinAlgWarning: The inner solver of NewtonCholeskySolver stumbled upon a singular or very ill-conditioned Hessian matrix at iteration #1. It will now resort to lbfgs instead.\n",
      "Further options are to use another solver or to avoid such situation in the first place. Possible remedies are removing collinear features of X or increasing the penalization strengths.\n",
      "The original Linear Algebra message was:\n",
      "Ill-conditioned matrix (rcond=8.55846e-18): result may not be accurate.\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "300 fits failed out of a total of 1080.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 1228, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_base.py\", line 1229, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='None' and loss='logistic_regression' is not supported, Parameters: penalty=None, loss='logistic_regression', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cholesky supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "60 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 1169, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.55791502        nan 0.55787522 0.55839773 0.55789827 0.55787522\n",
      "        nan 0.55854746        nan        nan        nan 0.55812009\n",
      " 0.55788476 0.55789095 0.55787413 0.55787413 0.55789745 0.55787413\n",
      " 0.5634645         nan 0.56345785 0.56333074 0.55925238 0.56344156\n",
      "        nan 0.56409479        nan        nan        nan 0.56415491\n",
      " 0.56353275 0.56355541 0.56356552 0.56356552 0.56359842 0.56353917\n",
      " 0.55791502        nan 0.55787522 0.55839773 0.55787522 0.55788172\n",
      "        nan 0.55839756        nan        nan        nan 0.55801662\n",
      " 0.55790626 0.55791939 0.55791939 0.55791939 0.55791939 0.55791939\n",
      " 0.5634645         nan 0.56345785 0.56333074 0.56184957 0.56345802\n",
      "        nan 0.56401387        nan        nan        nan 0.56412578\n",
      " 0.5635324  0.56356168 0.56354522 0.56354522 0.56352909 0.56353883\n",
      " 0.55791502        nan 0.55787522 0.55839773 0.55788172 0.55787522\n",
      "        nan 0.55830849        nan        nan        nan 0.55806186\n",
      " 0.5579588  0.55799186 0.55799186 0.55799186 0.55799837 0.55799186\n",
      " 0.5634645         nan 0.56345785 0.56333074 0.56483652 0.56344138\n",
      "        nan 0.56387083        nan        nan        nan 0.56399583\n",
      " 0.56353868 0.56353852 0.56353852 0.56353852 0.56354831 0.56355507]\n",
      "  warnings.warn(\n",
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_logistic.py:1193: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([9.41329455e-01, 7.65149355e-02, 2.32459130e+00, 4.97434998e-01,\n",
      "       6.47546725e+00, 2.86682174e+00, 1.88551664e-02, 2.69015002e-01,\n",
      "       1.95417166e-02, 2.14356899e-02, 2.46770144e-02, 3.87624147e+00,\n",
      "       3.55729556e-01, 2.14960790e-01, 1.29619439e+00, 2.92886925e-01,\n",
      "       3.29758127e+00, 2.46565969e+00, 3.62720108e-01, 5.50386190e-02,\n",
      "       1.86584210e+00, 4.74637556e-01, 2.01817772e+01, 4.35011346e+00,\n",
      "       2.10026503e-02, 3.91178393e-01, 2.29386806e-02, 2.08919764e-02,\n",
      "       1.84220076e-02, 4.71732492e+00, 4.90724874e-01, 2.96050620e-01,\n",
      "       1.53188560e+00, 3.08447456e-01, 3.62922378e+00, 2.81411669e+00,\n",
      "       4.37503695e-01, 8.49417210e-02, 2.23656476e+00, 5.88078070e-01,\n",
      "       6.06415236e+00, 2.60454311e+00, 1.87768936e-02, 3.00688314e-01,\n",
      "       2.52627850e-02, 2.01507092e-02, 2.48186350e-02, 3.41417630e+00,\n",
      "       3.78334618e-01, 2.83019733e-01, 1.46803820e+00, 3.42750049e-01,\n",
      "       2.85794132e+00, 2.77780578e+00, 4.77379203e-01, 6.82275772e-02,\n",
      "       1.96894448e+00, 5.24180555e-01, 1.95077730e+01, 3.65313823e+00,\n",
      "       2.75046587e-02, 5.06934428e-01, 2.60982513e-02, 2.13132143e-02,\n",
      "       2.40448952e-02, 4.93335619e+00, 3.61420465e-01, 2.33806801e-01,\n",
      "       1.24825578e+00, 2.85972977e-01, 3.95108378e+00, 2.80584755e+00,\n",
      "       3.63778973e-01, 5.90290785e-02, 2.15784619e+00, 5.14029694e-01,\n",
      "       5.84564202e+00, 2.60638747e+00, 1.94367409e-02, 3.06664324e-01,\n",
      "       1.90412045e-02, 2.07431555e-02, 1.88538074e-02, 3.31116657e+00,\n",
      "       4.14761734e-01, 2.78035235e-01, 1.59918256e+00, 3.74512601e-01,\n",
      "       3.31956301e+00, 2.78736043e+00, 3.97431159e-01, 6.06047869e-02,\n",
      "       2.22872610e+00, 6.11871982e-01, 2.15876488e+01, 3.47725031e+00,\n",
      "       2.07598925e-02, 4.74207401e-01, 1.79584742e-02, 1.86300039e-02,\n",
      "       1.90999508e-02, 5.11467679e+00, 4.55274749e-01, 2.76748490e-01,\n",
      "       1.63587294e+00, 3.50765085e-01, 4.65230284e+00, 2.70071766e+00]), 'std_fit_time': array([2.01412214e-01, 1.12817464e-02, 2.95651528e-01, 1.04181933e-01,\n",
      "       2.88834496e-01, 1.96087272e-01, 1.28744919e-03, 3.17685658e-02,\n",
      "       3.23712461e-03, 3.92300852e-03, 6.15960970e-03, 1.23001988e-01,\n",
      "       2.33619510e-02, 2.75546696e-02, 1.02427464e-01, 1.65434043e-02,\n",
      "       3.18931287e-01, 2.59375178e-01, 1.80671165e-02, 1.73974908e-02,\n",
      "       2.47919531e-01, 6.55005659e-02, 1.54718735e+00, 3.72031333e-01,\n",
      "       3.19248650e-03, 6.71399447e-02, 5.85536291e-03, 4.70284000e-03,\n",
      "       8.32062008e-04, 6.42743892e-01, 5.17832233e-02, 5.16503416e-02,\n",
      "       1.38089719e-01, 3.75950020e-02, 1.99530889e-01, 1.95507552e-01,\n",
      "       9.36587903e-02, 1.67300898e-02, 1.74161152e-01, 7.39581752e-02,\n",
      "       2.20262616e-01, 1.93466915e-01, 1.41188303e-03, 3.53799680e-02,\n",
      "       1.13967344e-02, 2.38152601e-03, 4.22008790e-03, 3.29014519e-01,\n",
      "       5.24534385e-02, 2.72779466e-02, 1.15527037e-01, 1.88477869e-02,\n",
      "       2.05616931e-01, 5.92407284e-01, 4.82227903e-02, 1.50984396e-02,\n",
      "       2.22870219e-01, 8.91269573e-02, 1.15940816e+00, 4.14139990e-01,\n",
      "       1.27551974e-02, 7.92439538e-02, 7.47372189e-03, 3.12973512e-03,\n",
      "       5.79865717e-03, 3.60215048e-01, 2.99925656e-02, 2.63202339e-02,\n",
      "       9.68489924e-02, 1.92558945e-02, 1.98790788e-01, 1.65621992e-01,\n",
      "       4.83451632e-02, 1.05849172e-02, 2.56811852e-01, 6.73246324e-02,\n",
      "       1.51903929e-01, 7.03906202e-02, 1.38065831e-03, 4.79625143e-02,\n",
      "       2.46390128e-03, 4.16267237e-03, 1.23252557e-03, 4.81277053e-01,\n",
      "       4.33201625e-02, 4.53835401e-02, 1.25388958e-01, 7.21165911e-02,\n",
      "       2.09631811e-01, 5.58544690e-01, 3.55095777e-02, 1.21014331e-02,\n",
      "       4.43265771e-01, 1.05016978e-01, 4.54869228e-01, 1.15051174e-01,\n",
      "       3.41658653e-03, 8.76812338e-02, 1.74840245e-03, 1.12922210e-03,\n",
      "       3.51480037e-03, 8.25099475e-01, 3.94331384e-02, 5.43360267e-02,\n",
      "       1.79095002e-01, 2.78815307e-02, 2.39143207e-01, 1.89921084e-01]), 'mean_score_time': array([0.0240469 , 0.        , 0.03362358, 0.00966539, 0.00918412,\n",
      "       0.00744414, 0.        , 0.00652902, 0.        , 0.        ,\n",
      "       0.        , 0.00770206, 0.00557353, 0.00388768, 0.00713882,\n",
      "       0.0041795 , 0.00520546, 0.00933702, 0.00661113, 0.        ,\n",
      "       0.02203021, 0.00882721, 0.01994712, 0.00582902, 0.        ,\n",
      "       0.00506988, 0.        , 0.        , 0.        , 0.00579264,\n",
      "       0.00649374, 0.00452831, 0.00821376, 0.00537834, 0.0063391 ,\n",
      "       0.00935512, 0.00747826, 0.        , 0.03267684, 0.01532543,\n",
      "       0.00670834, 0.00630469, 0.        , 0.00400827, 0.        ,\n",
      "       0.        , 0.        , 0.00961637, 0.00682335, 0.00573277,\n",
      "       0.00973051, 0.00583115, 0.00490398, 0.01766725, 0.0086941 ,\n",
      "       0.        , 0.03521211, 0.01010206, 0.01454394, 0.01083312,\n",
      "       0.        , 0.00695145, 0.        , 0.        , 0.        ,\n",
      "       0.00506833, 0.00503426, 0.00538495, 0.00474873, 0.0044693 ,\n",
      "       0.00596838, 0.01220052, 0.00407844, 0.        , 0.03075538,\n",
      "       0.01500227, 0.00633581, 0.00482497, 0.        , 0.00381634,\n",
      "       0.        , 0.        , 0.        , 0.00837488, 0.00953977,\n",
      "       0.00629604, 0.00931237, 0.00447941, 0.00692668, 0.01359673,\n",
      "       0.00438054, 0.        , 0.0302655 , 0.01491218, 0.0087971 ,\n",
      "       0.00386553, 0.        , 0.00453444, 0.        , 0.        ,\n",
      "       0.        , 0.00676248, 0.00510755, 0.0046278 , 0.00817146,\n",
      "       0.00617588, 0.00516279, 0.00478959]), 'std_score_time': array([0.02171991, 0.        , 0.02407283, 0.00459748, 0.00399523,\n",
      "       0.00404041, 0.        , 0.00319564, 0.        , 0.        ,\n",
      "       0.        , 0.00268428, 0.00208101, 0.00073229, 0.00443706,\n",
      "       0.00107343, 0.00199963, 0.00739732, 0.0053645 , 0.        ,\n",
      "       0.00950521, 0.0045489 , 0.01419654, 0.00187492, 0.        ,\n",
      "       0.00302987, 0.        , 0.        , 0.        , 0.00269374,\n",
      "       0.00201186, 0.00090049, 0.00453987, 0.00258796, 0.00212217,\n",
      "       0.00683811, 0.00584441, 0.        , 0.01366333, 0.01012102,\n",
      "       0.00230067, 0.00423113, 0.        , 0.00116806, 0.        ,\n",
      "       0.        , 0.        , 0.00762224, 0.0030342 , 0.00177681,\n",
      "       0.00747081, 0.00247424, 0.0017584 , 0.02263849, 0.00489277,\n",
      "       0.        , 0.02160738, 0.00492647, 0.00733276, 0.0083323 ,\n",
      "       0.        , 0.00333972, 0.        , 0.        , 0.        ,\n",
      "       0.00186758, 0.0027583 , 0.00159859, 0.00223301, 0.00289988,\n",
      "       0.00240701, 0.01930193, 0.00129352, 0.        , 0.02259644,\n",
      "       0.00808914, 0.00347783, 0.00171417, 0.        , 0.00080835,\n",
      "       0.        , 0.        , 0.        , 0.00779034, 0.00710226,\n",
      "       0.00230632, 0.00370561, 0.00129426, 0.00362063, 0.00989316,\n",
      "       0.00099884, 0.        , 0.01655122, 0.00806858, 0.00277637,\n",
      "       0.00053239, 0.        , 0.00174606, 0.        , 0.        ,\n",
      "       0.        , 0.00406674, 0.00163354, 0.00155254, 0.00578623,\n",
      "       0.0025787 , 0.00179907, 0.0016585 ]), 'param_C': masked_array(data=[0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
      "                   0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
      "                   0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2,\n",
      "                   0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,\n",
      "                   0.25, 0.25, 0.25, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_class_weight': masked_array(data=[{0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47}, 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47}, 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47},\n",
      "                   {0: 0.53, 1: 0.47}, {0: 0.53, 1: 0.47}, 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced', 'balanced', 'balanced', 'balanced',\n",
      "                   'balanced'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_max_iter': masked_array(data=[200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
      "                   200, 200, 200, 200, 200, 200, 200, 200, 200],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_penalty': masked_array(data=[None, None, None, None, None, None, 'l1', 'l1', 'l1',\n",
      "                   'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
      "                   None, None, None, None, None, None, 'l1', 'l1', 'l1',\n",
      "                   'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
      "                   None, None, None, None, None, None, 'l1', 'l1', 'l1',\n",
      "                   'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
      "                   None, None, None, None, None, None, 'l1', 'l1', 'l1',\n",
      "                   'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
      "                   None, None, None, None, None, None, 'l1', 'l1', 'l1',\n",
      "                   'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2',\n",
      "                   None, None, None, None, None, None, 'l1', 'l1', 'l1',\n",
      "                   'l1', 'l1', 'l1', 'l2', 'l2', 'l2', 'l2', 'l2', 'l2'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_solver': masked_array(data=['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky',\n",
      "                   'sag', 'saga', 'lbfgs', 'liblinear', 'newton-cg',\n",
      "                   'newton-cholesky', 'sag', 'saga', 'lbfgs', 'liblinear',\n",
      "                   'newton-cg', 'newton-cholesky', 'sag', 'saga', 'lbfgs',\n",
      "                   'liblinear', 'newton-cg', 'newton-cholesky', 'sag',\n",
      "                   'saga', 'lbfgs', 'liblinear', 'newton-cg',\n",
      "                   'newton-cholesky', 'sag', 'saga', 'lbfgs', 'liblinear',\n",
      "                   'newton-cg', 'newton-cholesky', 'sag', 'saga', 'lbfgs',\n",
      "                   'liblinear', 'newton-cg', 'newton-cholesky', 'sag',\n",
      "                   'saga', 'lbfgs', 'liblinear', 'newton-cg',\n",
      "                   'newton-cholesky', 'sag', 'saga', 'lbfgs', 'liblinear',\n",
      "                   'newton-cg', 'newton-cholesky', 'sag', 'saga', 'lbfgs',\n",
      "                   'liblinear', 'newton-cg', 'newton-cholesky', 'sag',\n",
      "                   'saga', 'lbfgs', 'liblinear', 'newton-cg',\n",
      "                   'newton-cholesky', 'sag', 'saga', 'lbfgs', 'liblinear',\n",
      "                   'newton-cg', 'newton-cholesky', 'sag', 'saga', 'lbfgs',\n",
      "                   'liblinear', 'newton-cg', 'newton-cholesky', 'sag',\n",
      "                   'saga', 'lbfgs', 'liblinear', 'newton-cg',\n",
      "                   'newton-cholesky', 'sag', 'saga', 'lbfgs', 'liblinear',\n",
      "                   'newton-cg', 'newton-cholesky', 'sag', 'saga', 'lbfgs',\n",
      "                   'liblinear', 'newton-cg', 'newton-cholesky', 'sag',\n",
      "                   'saga', 'lbfgs', 'liblinear', 'newton-cg',\n",
      "                   'newton-cholesky', 'sag', 'saga', 'lbfgs', 'liblinear',\n",
      "                   'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'lbfgs'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'liblinear'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'newton-cg'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'newton-cholesky'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'sag'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'saga'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cholesky'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cholesky'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}, {'C': 0.2, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'lbfgs'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'liblinear'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'newton-cg'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'newton-cholesky'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'sag'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'saga'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cholesky'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cholesky'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}, {'C': 0.2, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'lbfgs'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'liblinear'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'newton-cg'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'newton-cholesky'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'sag'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'saga'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cholesky'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cholesky'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}, {'C': 0.25, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'lbfgs'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'liblinear'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'newton-cg'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'newton-cholesky'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'sag'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'saga'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cholesky'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cholesky'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}, {'C': 0.25, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'lbfgs'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'liblinear'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'newton-cg'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'newton-cholesky'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'sag'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': None, 'solver': 'saga'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cholesky'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cholesky'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}, {'C': 0.3, 'class_weight': {0: 0.53, 1: 0.47}, 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'lbfgs'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'liblinear'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'newton-cg'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'newton-cholesky'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'sag'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': None, 'solver': 'saga'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'lbfgs'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'liblinear'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cg'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'newton-cholesky'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'sag'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l1', 'solver': 'saga'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'lbfgs'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'liblinear'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cg'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'newton-cholesky'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'sag'}, {'C': 0.3, 'class_weight': 'balanced', 'max_iter': 200, 'penalty': 'l2', 'solver': 'saga'}], 'split0_test_score': array([0.56753351,        nan, 0.5674685 , 0.56796061, 0.56753351,\n",
      "       0.5674685 ,        nan, 0.56655212,        nan,        nan,\n",
      "              nan, 0.56661702, 0.5674035 , 0.5674035 , 0.5674035 ,\n",
      "       0.5674035 , 0.5674685 , 0.5674035 , 0.57291549,        nan,\n",
      "       0.57285068, 0.57323689, 0.51870262, 0.57285068,        nan,\n",
      "       0.57100759,        nan,        nan,        nan, 0.57081351,\n",
      "       0.57356101, 0.57362588, 0.57362588, 0.57362588, 0.57362588,\n",
      "       0.57362588, 0.56753351,        nan, 0.5674685 , 0.56796061,\n",
      "       0.5674685 , 0.56753351,        nan, 0.56691471,        nan,\n",
      "              nan,        nan, 0.56648723, 0.56753351, 0.56773159,\n",
      "       0.56773159, 0.56773159, 0.56773159, 0.56773159, 0.57291549,\n",
      "              nan, 0.57285068, 0.57323689, 0.56809872, 0.57285068,\n",
      "              nan, 0.57178442,        nan,        nan,        nan,\n",
      "       0.57210777, 0.57362588, 0.57362588, 0.57362588, 0.57362588,\n",
      "       0.57346454, 0.57362588, 0.56753351,        nan, 0.5674685 ,\n",
      "       0.56796061, 0.56753351, 0.5674685 ,        nan, 0.56720861,\n",
      "              nan,        nan,        nan, 0.56691109, 0.56786164,\n",
      "       0.56786164, 0.56786164, 0.56786164, 0.56786164, 0.56786164,\n",
      "       0.57291549,        nan, 0.57285068, 0.57323689, 0.57332882,\n",
      "       0.57285068,        nan, 0.57214043,        nan,        nan,\n",
      "              nan, 0.57201087, 0.57362588, 0.57362588, 0.57362588,\n",
      "       0.57362588, 0.57356101, 0.57362588]), 'split1_test_score': array([0.55381424,        nan, 0.55381424, 0.55408363, 0.55381424,\n",
      "       0.55381424,        nan, 0.5547748 ,        nan,        nan,\n",
      "              nan, 0.55430021, 0.55358378, 0.55358378, 0.55358378,\n",
      "       0.55358378, 0.55358378, 0.55358378, 0.55744051,        nan,\n",
      "       0.55744051, 0.55738824, 0.55869343, 0.55744051,        nan,\n",
      "       0.5599909 ,        nan,        nan,        nan, 0.5599909 ,\n",
      "       0.55741436, 0.55741436, 0.55741436, 0.55741436, 0.55741436,\n",
      "       0.55741436, 0.55381424,        nan, 0.55381424, 0.55408363,\n",
      "       0.55381424, 0.55381424,        nan, 0.55385324,        nan,\n",
      "              nan,        nan, 0.55348087, 0.55358378, 0.55351999,\n",
      "       0.55351999, 0.55351999, 0.55351999, 0.55351999, 0.55744051,\n",
      "              nan, 0.55744051, 0.55738824, 0.54218988, 0.55744051,\n",
      "              nan, 0.55920828,        nan,        nan,        nan,\n",
      "       0.55953599, 0.55708594, 0.55708594, 0.55708594, 0.55708594,\n",
      "       0.55708594, 0.55708594, 0.55381424,        nan, 0.55381424,\n",
      "       0.55408363, 0.55381424, 0.55381424,        nan, 0.55355909,\n",
      "              nan,        nan,        nan, 0.55341708, 0.55375043,\n",
      "       0.55391705, 0.55391705, 0.55391705, 0.55391705, 0.55391705,\n",
      "       0.55744051,        nan, 0.55744051, 0.55738824, 0.5598908 ,\n",
      "       0.55744051,        nan, 0.55901751,        nan,        nan,\n",
      "              nan, 0.55934516, 0.55725017, 0.55725017, 0.55725017,\n",
      "       0.55725017, 0.55725017, 0.55725017]), 'split2_test_score': array([0.54933641,        nan, 0.54916898, 0.55024255, 0.54916898,\n",
      "       0.54916898,        nan, 0.55132641,        nan,        nan,\n",
      "              nan, 0.55034642, 0.54939982, 0.5493592 , 0.5493592 ,\n",
      "       0.5493592 , 0.5493592 , 0.5493592 , 0.55639098,        nan,\n",
      "       0.55639098, 0.55691011, 0.56228804, 0.55639098,        nan,\n",
      "       0.55737705,        nan,        nan,        nan, 0.55737705,\n",
      "       0.55645437, 0.55635328, 0.55628988, 0.55628988, 0.55645437,\n",
      "       0.55628988, 0.54933641,        nan, 0.54916898, 0.55024255,\n",
      "       0.54916898, 0.54916898,        nan, 0.55185142,        nan,\n",
      "              nan,        nan, 0.55170822, 0.54929577, 0.54946323,\n",
      "       0.54946323, 0.54946323, 0.54946323, 0.54946323, 0.55639098,\n",
      "              nan, 0.55639098, 0.55691011, 0.53635393, 0.55639098,\n",
      "              nan, 0.55737705,        nan,        nan,        nan,\n",
      "       0.55725017, 0.5565812 , 0.55651778, 0.55651778, 0.55651778,\n",
      "       0.55651778, 0.55651778, 0.54933641,        nan, 0.54916898,\n",
      "       0.55024255, 0.54916898, 0.54916898,        nan, 0.55226461,\n",
      "              nan,        nan,        nan, 0.55182751, 0.5493592 ,\n",
      "       0.5493592 , 0.5493592 , 0.5493592 , 0.5493592 , 0.5493592 ,\n",
      "       0.55639098,        nan, 0.55639098, 0.55691011, 0.56465854,\n",
      "       0.55639098,        nan, 0.55760474,        nan,        nan,\n",
      "              nan, 0.55803368, 0.55651778, 0.55651778, 0.55651778,\n",
      "       0.55651778, 0.55651778, 0.55651778]), 'split3_test_score': array([0.55928514,        nan, 0.55928514, 0.55962463, 0.55928514,\n",
      "       0.55928514,        nan, 0.55924062,        nan,        nan,\n",
      "              nan, 0.55877303, 0.55924822, 0.55941331, 0.55941331,\n",
      "       0.55941331, 0.55941331, 0.55941331, 0.56457815,        nan,\n",
      "       0.56457815, 0.56532067, 0.56866276, 0.56457815,        nan,\n",
      "       0.56460738,        nan,        nan,        nan, 0.56493727,\n",
      "       0.56493213, 0.56486823, 0.56486823, 0.56486823, 0.56486823,\n",
      "       0.56486823, 0.55928514,        nan, 0.55928514, 0.55962463,\n",
      "       0.55928514, 0.55928514,        nan, 0.55874614,        nan,\n",
      "              nan,        nan, 0.55815018, 0.55885387, 0.55901902,\n",
      "       0.55901902, 0.55901902, 0.55901902, 0.55901902, 0.56457815,\n",
      "              nan, 0.56457815, 0.56532067, 0.56807935, 0.56457815,\n",
      "              nan, 0.56509445,        nan,        nan,        nan,\n",
      "       0.56525673, 0.56490276, 0.56496664, 0.56496664, 0.56496664,\n",
      "       0.56496664, 0.56496664, 0.55928514,        nan, 0.55928514,\n",
      "       0.55962463, 0.55928514, 0.55928514,        nan, 0.55826186,\n",
      "              nan,        nan,        nan, 0.55793206, 0.55885387,\n",
      "       0.55885387, 0.55885387, 0.55885387, 0.55885387, 0.55885387,\n",
      "       0.56457815,        nan, 0.56457815, 0.56532067, 0.56618729,\n",
      "       0.56457815,        nan, 0.56428248,        nan,        nan,\n",
      "              nan, 0.56476977, 0.56496664, 0.56496664, 0.56496664,\n",
      "       0.56496664, 0.56496664, 0.56496664]), 'split4_test_score': array([0.55280586,        nan, 0.55280586, 0.5540572 , 0.55280586,\n",
      "       0.55280586,        nan, 0.55111524,        nan,        nan,\n",
      "              nan, 0.55052913, 0.55230126, 0.55246949, 0.55230126,\n",
      "       0.55230126, 0.55246949, 0.55230126, 0.5585027 ,        nan,\n",
      "       0.5585027 , 0.55831038, 0.56075195, 0.5585027 ,        nan,\n",
      "       0.5562579 ,        nan,        nan,        nan, 0.55635988,\n",
      "       0.55843858, 0.55843858, 0.55843858, 0.55843858, 0.55843858,\n",
      "       0.55843858, 0.55280586,        nan, 0.55280586, 0.5540572 ,\n",
      "       0.55280586, 0.55280586,        nan, 0.5521729 ,        nan,\n",
      "              nan,        nan, 0.55169206, 0.55280586, 0.55263769,\n",
      "       0.55263769, 0.55263769, 0.55263769, 0.55263769, 0.5585027 ,\n",
      "              nan, 0.5585027 , 0.55831038, 0.55847145, 0.5585027 ,\n",
      "              nan, 0.55715106,        nan,        nan,        nan,\n",
      "       0.55706085, 0.55837447, 0.55837447, 0.55837447, 0.55837447,\n",
      "       0.55837447, 0.55837447, 0.55280586,        nan, 0.55280586,\n",
      "       0.5540572 , 0.55280586, 0.55280586,        nan, 0.55234112,\n",
      "              nan,        nan,        nan, 0.55213298, 0.55263769,\n",
      "       0.55270192, 0.55270192, 0.55270192, 0.55270192, 0.55270192,\n",
      "       0.5585027 ,        nan, 0.5585027 , 0.55831038, 0.54843238,\n",
      "       0.5585027 ,        nan, 0.55685734,        nan,        nan,\n",
      "              nan, 0.55685734, 0.55837447, 0.55820896, 0.55820896,\n",
      "       0.55820896, 0.55820896, 0.55837447]), 'split5_test_score': array([0.56029429,        nan, 0.56012877, 0.55920147, 0.56029429,\n",
      "       0.56012877,        nan, 0.56161709,        nan,        nan,\n",
      "              nan, 0.56121393, 0.5610766 , 0.56094756, 0.56094756,\n",
      "       0.56094756, 0.56094756, 0.56094756, 0.56487683,        nan,\n",
      "       0.56503973, 0.565     , 0.55519329, 0.56487683,        nan,\n",
      "       0.56828494,        nan,        nan,        nan, 0.5683184 ,\n",
      "       0.56562677, 0.56562677, 0.56562677, 0.56562677, 0.56562677,\n",
      "       0.56562677, 0.56029429,        nan, 0.56012877, 0.55920147,\n",
      "       0.56012877, 0.56012877,        nan, 0.56062522,        nan,\n",
      "              nan,        nan, 0.56032202, 0.56078206, 0.56078206,\n",
      "       0.56078206, 0.56078206, 0.56078206, 0.56078206, 0.56487683,\n",
      "              nan, 0.56503973, 0.565     , 0.55365655, 0.56487683,\n",
      "              nan, 0.56734694,        nan,        nan,        nan,\n",
      "       0.56734694, 0.5655282 , 0.5655282 , 0.5655282 , 0.5655282 ,\n",
      "       0.5655282 , 0.5655282 , 0.56029429,        nan, 0.56012877,\n",
      "       0.55920147, 0.56012877, 0.56012877,        nan, 0.56102882,\n",
      "              nan,        nan,        nan, 0.56082711, 0.56075411,\n",
      "       0.56075411, 0.56075411, 0.56075411, 0.56075411, 0.56075411,\n",
      "       0.56487683,        nan, 0.56503973, 0.565     , 0.56757063,\n",
      "       0.56503973,        nan, 0.56747562,        nan,        nan,\n",
      "              nan, 0.5675737 , 0.56536541, 0.56536541, 0.56536541,\n",
      "       0.56536541, 0.5655282 , 0.56536541]), 'split6_test_score': array([0.5540775 ,        nan, 0.5540775 , 0.5545402 , 0.5540775 ,\n",
      "       0.5540775 ,        nan, 0.55542725,        nan,        nan,\n",
      "              nan, 0.55451497, 0.55357556, 0.55363963, 0.55363963,\n",
      "       0.55363963, 0.55363963, 0.55363963, 0.55905872,        nan,\n",
      "       0.55889409, 0.55837332, 0.57155615, 0.55889409,        nan,\n",
      "       0.56142352,        nan,        nan,        nan, 0.56152355,\n",
      "       0.55945174, 0.55945174, 0.55961626, 0.55961626, 0.55978075,\n",
      "       0.55945174, 0.5540775 ,        nan, 0.5540775 , 0.5545402 ,\n",
      "       0.5540775 , 0.5540775 ,        nan, 0.55506811,        nan,\n",
      "              nan,        nan, 0.55452865, 0.55420572, 0.55420572,\n",
      "       0.55420572, 0.55420572, 0.55420572, 0.55420572, 0.55905872,\n",
      "              nan, 0.55889409, 0.55837332, 0.55640314, 0.55905872,\n",
      "              nan, 0.56115929,        nan,        nan,        nan,\n",
      "       0.56115929, 0.55928718, 0.55957957, 0.55941499, 0.55941499,\n",
      "       0.55941499, 0.55935108, 0.5540775 ,        nan, 0.5540775 ,\n",
      "       0.5545402 , 0.5540775 , 0.5540775 ,        nan, 0.55450346,\n",
      "              nan,        nan,        nan, 0.55436164, 0.55439815,\n",
      "       0.55433399, 0.55433399, 0.55433399, 0.55433399, 0.55433399,\n",
      "       0.55905872,        nan, 0.55889409, 0.55837332, 0.56124314,\n",
      "       0.55872943,        nan, 0.5608031 ,        nan,        nan,\n",
      "              nan, 0.56096726, 0.55918647, 0.55918647, 0.55918647,\n",
      "       0.55918647, 0.55918647, 0.55918647]), 'split7_test_score': array([0.55629443,        nan, 0.55629443, 0.5576659 , 0.55629443,\n",
      "       0.55629443,        nan, 0.55628077,        nan,        nan,\n",
      "              nan, 0.55659945, 0.55629443, 0.55612888, 0.55612888,\n",
      "       0.55612888, 0.55612888, 0.55612888, 0.56431723,        nan,\n",
      "       0.56431723, 0.56357466, 0.55870909, 0.56431723,        nan,\n",
      "       0.56416629,        nan,        nan,        nan, 0.56426474,\n",
      "       0.5638358 , 0.5638358 , 0.5638358 , 0.5638358 , 0.5638358 ,\n",
      "       0.5638358 , 0.55629443,        nan, 0.55629443, 0.5576659 ,\n",
      "       0.55629443, 0.55629443,        nan, 0.55615341,        nan,\n",
      "              nan,        nan, 0.55603942, 0.55589955, 0.5559633 ,\n",
      "       0.5559633 , 0.5559633 , 0.5559633 , 0.5559633 , 0.56431723,\n",
      "              nan, 0.56431723, 0.56357466, 0.58716187, 0.56431723,\n",
      "              nan, 0.5640678 ,        nan,        nan,        nan,\n",
      "       0.56423003, 0.56389957, 0.56389957, 0.56389957, 0.56389957,\n",
      "       0.56389957, 0.56389957, 0.55629443,        nan, 0.55629443,\n",
      "       0.5576659 , 0.55629443, 0.55629443,        nan, 0.55667506,\n",
      "              nan,        nan,        nan, 0.55647194, 0.55623065,\n",
      "       0.55629443, 0.55629443, 0.55629443, 0.55629443, 0.55629443,\n",
      "       0.56431723,        nan, 0.56431723, 0.56357466, 0.56702307,\n",
      "       0.56431723,        nan, 0.56360963,        nan,        nan,\n",
      "              nan, 0.56348219, 0.56412576, 0.56412576, 0.56412576,\n",
      "       0.56412576, 0.56412576, 0.56412576]), 'split8_test_score': array([0.56451613,        nan, 0.56451613, 0.56517734, 0.56451613,\n",
      "       0.56451613,        nan, 0.56922368,        nan,        nan,\n",
      "              nan, 0.56876081, 0.56530753, 0.56524243, 0.56524243,\n",
      "       0.56524243, 0.56524243, 0.56524243, 0.57087556,        nan,\n",
      "       0.57087556, 0.56996587, 0.5727989 , 0.57087556,        nan,\n",
      "       0.57172131,        nan,        nan,        nan, 0.57175373,\n",
      "       0.57077782, 0.57094056, 0.57094056, 0.57094056, 0.57094056,\n",
      "       0.57100558, 0.56451613,        nan, 0.56451613, 0.56517734,\n",
      "       0.56451613, 0.56451613,        nan, 0.56853493,        nan,\n",
      "              nan,        nan, 0.56813731, 0.56524243, 0.56524243,\n",
      "       0.56524243, 0.56524243, 0.56524243, 0.56524243, 0.57087556,\n",
      "              nan, 0.57087556, 0.56996587, 0.56917448, 0.57087556,\n",
      "              nan, 0.57149362,        nan,        nan,        nan,\n",
      "       0.57159117, 0.57094056, 0.57094056, 0.57094056, 0.57094056,\n",
      "       0.57094056, 0.57094056, 0.56451613,        nan, 0.56451613,\n",
      "       0.56517734, 0.56451613, 0.56451613,        nan, 0.56777906,\n",
      "              nan,        nan,        nan, 0.56758001, 0.56494703,\n",
      "       0.5650472 , 0.5650472 , 0.5650472 , 0.56511226, 0.5650472 ,\n",
      "       0.57087556,        nan, 0.57087556, 0.56996587, 0.56996354,\n",
      "       0.57087556,        nan, 0.5715262 ,        nan,        nan,\n",
      "              nan, 0.57165623, 0.57094056, 0.57094056, 0.57094056,\n",
      "       0.57094056, 0.57094056, 0.57094056]), 'split9_test_score': array([0.56119265,        nan, 0.56119265, 0.56142378, 0.56119265,\n",
      "       0.56119265,        nan, 0.55991664,        nan,        nan,\n",
      "              nan, 0.55954588, 0.56065688, 0.56072172, 0.56072172,\n",
      "       0.56072172, 0.56072172, 0.56072172, 0.56568885,        nan,\n",
      "       0.56568885, 0.56522732, 0.56516751, 0.56568885,        nan,\n",
      "       0.56611098,        nan,        nan,        nan, 0.56621005,\n",
      "       0.56483491, 0.56499886, 0.56499886, 0.56499886, 0.56499886,\n",
      "       0.56483491, 0.56119265,        nan, 0.56119265, 0.56142378,\n",
      "       0.56119265, 0.56119265,        nan, 0.56005554,        nan,\n",
      "              nan,        nan, 0.55962028, 0.56086002, 0.56062883,\n",
      "       0.56062883, 0.56062883, 0.56062883, 0.56062883, 0.56568885,\n",
      "              nan, 0.56568885, 0.56522732, 0.57890634, 0.56568885,\n",
      "              nan, 0.56545579,        nan,        nan,        nan,\n",
      "       0.56571885, 0.56509822, 0.56509822, 0.56509822, 0.56509822,\n",
      "       0.56509822, 0.56509822, 0.56119265,        nan, 0.56119265,\n",
      "       0.56142378, 0.56119265, 0.56119265,        nan, 0.55946321,\n",
      "              nan,        nan,        nan, 0.55915721, 0.56079519,\n",
      "       0.56079519, 0.56079519, 0.56079519, 0.56079519, 0.56079519,\n",
      "       0.56568885,        nan, 0.56568885, 0.56522732, 0.57006695,\n",
      "       0.56568885,        nan, 0.56539121,        nan,        nan,\n",
      "              nan, 0.56526208, 0.56503369, 0.56519753, 0.56519753,\n",
      "       0.56519753, 0.56519753, 0.56519753]), 'mean_test_score': array([0.55791502,        nan, 0.55787522, 0.55839773, 0.55789827,\n",
      "       0.55787522,        nan, 0.55854746,        nan,        nan,\n",
      "              nan, 0.55812009, 0.55788476, 0.55789095, 0.55787413,\n",
      "       0.55787413, 0.55789745, 0.55787413, 0.5634645 ,        nan,\n",
      "       0.56345785, 0.56333074, 0.55925238, 0.56344156,        nan,\n",
      "       0.56409479,        nan,        nan,        nan, 0.56415491,\n",
      "       0.56353275, 0.56355541, 0.56356552, 0.56356552, 0.56359842,\n",
      "       0.56353917, 0.55791502,        nan, 0.55787522, 0.55839773,\n",
      "       0.55787522, 0.55788172,        nan, 0.55839756,        nan,\n",
      "              nan,        nan, 0.55801662, 0.55790626, 0.55791939,\n",
      "       0.55791939, 0.55791939, 0.55791939, 0.55791939, 0.5634645 ,\n",
      "              nan, 0.56345785, 0.56333074, 0.56184957, 0.56345802,\n",
      "              nan, 0.56401387,        nan,        nan,        nan,\n",
      "       0.56412578, 0.5635324 , 0.56356168, 0.56354522, 0.56354522,\n",
      "       0.56352909, 0.56353883, 0.55791502,        nan, 0.55787522,\n",
      "       0.55839773, 0.55788172, 0.55787522,        nan, 0.55830849,\n",
      "              nan,        nan,        nan, 0.55806186, 0.5579588 ,\n",
      "       0.55799186, 0.55799186, 0.55799186, 0.55799837, 0.55799186,\n",
      "       0.5634645 ,        nan, 0.56345785, 0.56333074, 0.56483652,\n",
      "       0.56344138,        nan, 0.56387083,        nan,        nan,\n",
      "              nan, 0.56399583, 0.56353868, 0.56353852, 0.56353852,\n",
      "       0.56353852, 0.56354831, 0.56355507]), 'std_test_score': array([0.00536876,        nan, 0.00537694, 0.00517779, 0.00539569,\n",
      "       0.00537694,        nan, 0.00571246,        nan,        nan,\n",
      "              nan, 0.00587713, 0.0055516 , 0.00553281, 0.0055495 ,\n",
      "       0.0055495 , 0.00554401, 0.0055495 , 0.00532455,        nan,\n",
      "       0.00533152, 0.00528089, 0.01459886, 0.00532691,        nan,\n",
      "       0.00508295,        nan,        nan,        nan, 0.00505199,\n",
      "       0.00539751, 0.00544735, 0.00544361, 0.00544361, 0.00540999,\n",
      "       0.00546048, 0.00536876,        nan, 0.00537694, 0.00517779,\n",
      "       0.00537694, 0.00538856,        nan, 0.00549101,        nan,\n",
      "              nan,        nan, 0.00546378, 0.00548594, 0.00550478,\n",
      "       0.00550478, 0.00550478, 0.00550478, 0.00550478, 0.00532455,\n",
      "              nan, 0.00533152, 0.00528089, 0.01485898, 0.00531307,\n",
      "              nan, 0.00503066,        nan,        nan,        nan,\n",
      "       0.00510589, 0.00547457, 0.00546231, 0.00547452, 0.00547452,\n",
      "       0.00544494, 0.00547937, 0.00536876,        nan, 0.00537694,\n",
      "       0.00517779, 0.00538856, 0.00537694,        nan, 0.00538034,\n",
      "              nan,        nan,        nan, 0.00538126, 0.00546996,\n",
      "       0.0054662 , 0.0054662 , 0.0054662 , 0.00547463, 0.0054662 ,\n",
      "       0.00532455,        nan, 0.00533152, 0.00528089, 0.0066809 ,\n",
      "       0.00534583,        nan, 0.00513764,        nan,        nan,\n",
      "              nan, 0.00505466, 0.00546758, 0.00548815, 0.00548815,\n",
      "       0.00548815, 0.00548191, 0.00547228]), 'rank_test_score': array([60, 79, 70, 41, 64, 70, 79, 40, 79, 79, 79, 46, 67, 66, 76, 76, 65,\n",
      "       76, 26, 79, 30, 35, 39, 33, 79,  4, 79, 79, 79,  2, 23, 12,  9,  9,\n",
      "        8, 17, 60, 79, 70, 41, 70, 68, 79, 44, 79, 79, 79, 48, 63, 55, 55,\n",
      "       55, 55, 55, 26, 79, 30, 35, 38, 29, 79,  5, 79, 79, 79,  3, 24, 11,\n",
      "       15, 15, 25, 18, 60, 79, 70, 41, 68, 70, 79, 45, 79, 79, 79, 47, 54,\n",
      "       50, 50, 50, 49, 50, 26, 79, 30, 35,  1, 34, 79,  7, 79, 79, 79,  6,\n",
      "       19, 20, 20, 20, 14, 13], dtype=int32)}\n",
      "<bound method BaseEstimator.get_params of LogisticRegression(C=0.3, class_weight='balanced', max_iter=200, penalty=None,\n",
      "                   solver='sag')>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankamenskiy/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, balanced_accuracy_score\n",
    "\n",
    "\n",
    "params_grid = {\n",
    "    'penalty': [None, 'l1', 'l2'],\n",
    "    'C': [0.2, 0.25, 0.3],\n",
    "    'class_weight': [\n",
    "        {\n",
    "        0: 0.53, \n",
    "        1: 0.47\n",
    "        },\n",
    "        'balanced'\n",
    "        ],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "    'max_iter': [200]\n",
    "}\n",
    "\n",
    "logreg_gridsearch = GridSearchCV(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid=params_grid,\n",
    "    scoring=make_scorer(f1_score),\n",
    "    cv=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "logreg_gridsearch.fit(X_train, y_train)\n",
    "logreg_model = logreg_gridsearch.best_estimator_\n",
    "\n",
    "print(logreg_gridsearch.cv_results_)\n",
    "print(logreg_model.get_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.57      0.55      4478\n",
      "           1       0.59      0.55      0.57      5003\n",
      "\n",
      "    accuracy                           0.56      9481\n",
      "   macro avg       0.56      0.56      0.56      9481\n",
      "weighted avg       0.56      0.56      0.56      9481\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x28988e760>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XXX, yyy = X_test, y_test\n",
    "XXX, yyy = X_val, y_val\n",
    "\n",
    "preds = logreg_model.predict(XXX)\n",
    "probas = logreg_model.predict_proba(XXX)\n",
    "\n",
    "# plot_prob_hist(probas)\n",
    "# compare_results(preds, yyy, XXX)\n",
    "print(classification_report(yyy, preds))\n",
    "\n",
    "sklearn.metrics.PrecisionRecallDisplay.from_estimator(\n",
    "    logreg_model, XXX.copy(), yyy.copy(), name=\"LinearSVC\", plot_chance_level=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.55      0.54      1990\n",
      "           1       0.55      0.51      0.53      2105\n",
      "\n",
      "    accuracy                           0.53      4095\n",
      "   macro avg       0.53      0.53      0.53      4095\n",
      "weighted avg       0.53      0.53      0.53      4095\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x28925fa30>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XXX, yyy = X_test, y_test\n",
    "# XXX, yyy = X_val, y_val\n",
    "\n",
    "preds = logreg_model.predict(XXX)\n",
    "probas = logreg_model.predict_proba(XXX)\n",
    "\n",
    "# plot_prob_hist(probas)\n",
    "# compare_results(preds, yyy, XXX)\n",
    "print(classification_report(yyy, preds))\n",
    "\n",
    "sklearn.metrics.PrecisionRecallDisplay.from_estimator(\n",
    "    logreg_model, XXX.copy(), yyy.copy(), name=\"LinearSVC\", plot_chance_level=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465f9c4753af46e1b1a6d67cedfb2f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2de4afc40>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "cbt_model = CatBoostClassifier(\n",
    "                iterations=15_000,                \n",
    "                learning_rate=6e-3,\n",
    "                depth=3\n",
    "            )\n",
    "\n",
    "cbt_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=(X_val, y_val),\n",
    "    plot=True,\n",
    "    silent=True,\n",
    "    use_best_model=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preds</th>\n",
       "      <th>prob_dire</th>\n",
       "      <th>prob_radiant</th>\n",
       "      <th>X_test</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.394255</td>\n",
       "      <td>0.605745</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.603765</td>\n",
       "      <td>0.396235</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.437307</td>\n",
       "      <td>0.562693</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.290635</td>\n",
       "      <td>0.709365</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.434701</td>\n",
       "      <td>0.565299</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4079</th>\n",
       "      <td>1</td>\n",
       "      <td>0.373473</td>\n",
       "      <td>0.626527</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>1</td>\n",
       "      <td>0.439276</td>\n",
       "      <td>0.560724</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>1</td>\n",
       "      <td>0.427637</td>\n",
       "      <td>0.572363</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4087</th>\n",
       "      <td>0</td>\n",
       "      <td>0.584680</td>\n",
       "      <td>0.415320</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4090</th>\n",
       "      <td>1</td>\n",
       "      <td>0.396204</td>\n",
       "      <td>0.603796</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1891 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      preds  prob_dire  prob_radiant  \\\n",
       "0         1   0.394255      0.605745   \n",
       "3         0   0.603765      0.396235   \n",
       "4         1   0.437307      0.562693   \n",
       "5         1   0.290635      0.709365   \n",
       "7         1   0.434701      0.565299   \n",
       "...     ...        ...           ...   \n",
       "4079      1   0.373473      0.626527   \n",
       "4084      1   0.439276      0.560724   \n",
       "4085      1   0.427637      0.572363   \n",
       "4087      0   0.584680      0.415320   \n",
       "4090      1   0.396204      0.603796   \n",
       "\n",
       "                                                 X_test  y_test  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0  \n",
       "4     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...       0  \n",
       "5     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0  \n",
       "7     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1  \n",
       "...                                                 ...     ...  \n",
       "4079  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1  \n",
       "4084  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0  \n",
       "4085  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1  \n",
       "4087  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1  \n",
       "4090  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1  \n",
       "\n",
       "[1891 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1891, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1888</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1890</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1891 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_pred  y_true\n",
       "0          1       0\n",
       "1          0       0\n",
       "2          1       0\n",
       "3          1       0\n",
       "4          1       1\n",
       "...      ...     ...\n",
       "1886       1       1\n",
       "1887       1       0\n",
       "1888       1       1\n",
       "1889       0       1\n",
       "1890       1       1\n",
       "\n",
       "[1891 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.33      0.41       894\n",
      "           1       0.56      0.77      0.65       997\n",
      "\n",
      "    accuracy                           0.56      1891\n",
      "   macro avg       0.56      0.55      0.53      1891\n",
      "weighted avg       0.56      0.56      0.54      1891\n",
      "\n",
      "Lost 0.538 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "THRESHOLD = 0.55\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "preds = cbt_model.predict(X_test)\n",
    "probas = cbt_model.predict_proba(X_test)\n",
    "\n",
    "print('Accuracy:', accuracy_score(y_test, preds))\n",
    "\n",
    "df['preds'] = preds.tolist()\n",
    "df['prob_dire'] = probas[:, 0].tolist()\n",
    "df['prob_radiant'] = probas[:, 1].tolist()\n",
    "df['X_test'] = X_test.tolist()\n",
    "df['y_test'] = y_test.tolist()\n",
    "\n",
    "df_thr = df[(df['prob_dire'] > THRESHOLD) | (df['prob_radiant'] > THRESHOLD)]\n",
    "display(df_thr)\n",
    "\n",
    "probas_thr = np.concatenate(\n",
    "    (\n",
    "        df_thr['prob_dire'].to_numpy().reshape(-1, 1), \n",
    "        df_thr['prob_radiant'].to_numpy().reshape(-1, 1)\n",
    "    ), axis=1)\n",
    "print(probas_thr.shape)\n",
    "preds_thr = df_thr['preds'].to_numpy()\n",
    "X_test_thr = df_thr['X_test'].to_numpy()\n",
    "y_test_thr = df_thr['y_test'].to_numpy()\n",
    "\n",
    "plot_prob_hist(probas_thr)\n",
    "compare_results(preds_thr, y_test_thr, X_test_thr)\n",
    "print(classification_report(y_test_thr, preds_thr))\n",
    "print(f'Lost {1 - df_thr.shape[0] / df.shape[0]:.3f} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 1 0 1]\n",
      "[1 0 0 ... 1 0 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_pred</th>\n",
       "      <th>y_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1982 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_pred  y_true\n",
       "0          1       1\n",
       "1          0       0\n",
       "2          0       0\n",
       "3          0       0\n",
       "4          1       1\n",
       "...      ...     ...\n",
       "1977       1       1\n",
       "1978       1       1\n",
       "1979       1       1\n",
       "1980       0       0\n",
       "1981       1       1\n",
       "\n",
       "[1982 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       946\n",
      "           1       0.92      0.91      0.91      1036\n",
      "\n",
      "    accuracy                           0.91      1982\n",
      "   macro avg       0.91      0.91      0.91      1982\n",
      "weighted avg       0.91      0.91      0.91      1982\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = cbt_model.predict(X_val)\n",
    "probas = cbt_model.predict_proba(X_val)\n",
    "\n",
    "plot_prob_hist(probas)\n",
    "compare_results(preds, y_val, X_val)\n",
    "print(classification_report(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HeroIds_0.91-F1.cbm'\n",
    "\n",
    "cbt_model.save_model(\n",
    "    fname='/Users/ankamenskiy/SmartDota/models/catboost/' + model_name,\n",
    "    format=\"cbm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "CatBoost:\n",
      " {'result': 'Radiant', 'dire': '0.28', 'radiant': '0.72'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from lib.results_view import make_single_prediction\n",
    "\n",
    "# print('LogReg:\\n', make_single_prediction(logreg_model, 7599378829))\n",
    "print('-'*50)\n",
    "print('CatBoost:\\n', make_single_prediction(cbt_model, 7599358486))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
